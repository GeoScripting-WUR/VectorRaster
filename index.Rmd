---
author: "Lo√Øc Dutrieux, Jan Verbesselt, Dainius Masiliunas and David Swinkels"
date: "`r format(Sys.time(), '%d %B, %Y')`"
pagetitle: "Raster Vector integration in R"
output:
  knitrBootstrap::bootstrap_document:
    theme: "simplex"
    highlight: Tomorrow Night Bright
    menu: FALSE
    theme.chooser: TRUE
    highlight.chooser: TRUE
---

# [WUR Geoscripting](https://geoscripting-wur.github.io/) <img src="https://www.wur.nl/upload/5e55a1d9-47c3-4149-9384-26d5147ee2d7_WUR_RGB_standard.png" alt="WUR logo" style="height: 35px;"/>

<!--
# Today

Time  | Activity
------------- | -------------
Until 11:00 | Review yesterday's exercise answer of the other team
Morning       |  Self-study: go through the following tutorial
14:00 to 15:00  | Discussion
Rest of the afternoon | Do and finalise the exercise
-->

# Week 2, Lesson 6: Raster - Vector integration in R

# Today's learning objectives

At the end of the lecture, you should be able to:

- Have an overview of what can be achieved when combining raster and vector data in R
- Transform simple feature objects of the *sf* package into spatial objects of the *sp* package
- Be able to extract raster data using vector data

# Introduction

In the previous lectures we saw how to deal with [raster](http://geoscripting-wur.github.io/IntroToRaster/) data using R, as well as how to deal with [vector](http://geoscripting-wur.github.io/IntroToVector/) data and the multiple classes of the *sf* package.

# Working with *sf*

As you discovered in the previous exercise, the *sf* (simple features) package provides a set of tools for working with geospatial vectors, i.e. points, lines and polygons. The *sf* package conforms to the open standard ISO 19125-1:2004, which is widely implemented in spatial databases and GIS software. Check back to remind yourself of the available classes *sf* offers if you need to [data classes in *sf* package](http://geoscripting-wur.github.io/IntroToVector/). With this link you will find a [handy 'cheatsheet'](https://github.com/rstudio/cheatsheets/blob/master/sf.pdf) for spatial operations with *sf*. The functions of the *sf* package are prefixed by `st_`, short for 'spatial type'.  

# Conversions

You will find some utilities in R to convert data from raster to vector format and vice-versa. However, whenever you start converting objects, you should wonder whether you are taking the right approach to solve your problem. An approach that does not involve converting your data from vector to raster or the opposite should almost always be preferred.

As a result, because these functions are only useful for some very particular situations, I only give a brief description of them below.

## Vector to raster

There is one function that allows to convert a vector object to a raster object. It is the `rasterize()` function.

## Raster to vector

Three functions allow to convert raster data to vector; the `rasterToPoints()`, `rasterToContour()`, and `rasterToPolygons()` functions. The latter can be useful to convert the result of a classification. In that case, set `dissolve =` to `TRUE` so that the polygons with the same attribute value will be dissolved into multi-polygon regions. This option requires the *rgeos* package. **Note**: These methods are known to perform poorly under R. Using the `terra::as.polygons()` etc. functions from the *terra* package, which is a sucessor of the *raster* package, is much faster due to *terra* being written in C++ instead. Calling `gdal_translate` directly or through the *gdalUtils* package can also be much faster for the same reason.

# Geometric operations

Raw raster data do not usually conform to any notion of administrative or geographical boundaries. Vector data (and extents) can be used to mask or crop data to a desired region of interest.

## Crop

Cropping consists in reducing the extent of a spatial object to a smaller extent. As a result, the output of `crop()` will automatically be rectangular and will not consider any features such as polygons to perform the subsetting. It is often useful to crop data as tightly as possible to the area under investigation to reduce the amount of data and have a more focused view when visualizing the data.

Crop uses objects of class `extent` to define the new extent, or any object that can be coerced to an extent (see `?extent` for more info on this). This means that practically all spatial objects (raster or vector) can be used directly in crop. Considering two raster objects `r1` and `r2` with `r2` smaller than `r1`, you can simply use `crop(r1, r2)` in order to crop `r1` to the extent of `r2`.

You can easily define an extent interactively (by clicking) thanks to the `drawExtent()` function.

## Mask

`mask()` can be used with almost all spatial objects to mask (= set to `NA`) values of a raster object. When used with a `SpatialPolygon` object, `mask` will keep values of the raster overlayed by polygons and mask the values outside of polygons.

Note the very useful `inverse=` argument of `mask()`, which allows to mask the inverse of the area covered by the features. We will use this feature of mask later in the tutorial to exclude water areas of a raster, defined in an independent `SpatialPolygons` object.

## Extract

The most common operation when combining vector and raster data is the extraction. It simply consists in extracting the values of a raster object for locations specified by a vector object. The object can be one of the classes of the *sp* package, or an `extent` object.

When using `extract()` with `SpatialPolygons` or `SpatialLines`, individual features of the vector layer may overlay or intersect several pixels. In that case a function (`fun =`) can be used to summarize the values into one. Note that although most often the function `min`, `max`, `mean` and `median` are used for the spatial aggregation, also any custom-made function can be used. `extract()` provides many options for the return object, such as a data frame or a new *sp* object with extracted values appended to the attribute table. See `?extract()` example section.

# Examples

## A simple land cover classification of Wageningen from Landsat 8 data

In the example below we will do a simple supervised land cover classification of Wageningen. The example uses the same data as you used in the [exercise](http://geoscripting-wur.github.io/IntroToRaster/#exercise-design-a-pre-processing-chain-to-assess-change-in-ndvi-over-time) of the raster lesson.

Step by step we will:

- Download the Landsat 8 data of Wageningen
- Download and prepare administrative boundary data of the Netherlands
- Download Water area data of Wageningen
- Mask the data to match the boundaries of the city
- Mask the data to exclude water bodies
- Build a calibration dataset using Google Maps
- Export the KML file and import it in R
- Extract the surface reflectance values for the calibration pixels
- Calibrate a model with the classifier
- Predict the land cover using a Landsat image

### Prepare the data

```{r, eval=FALSE}
if(!"raster" %in% rownames(installed.packages())){install.packages("raster")}
if(!"sf" %in% rownames(installed.packages())){install.packages("sf")}

library(raster)
library(sf)
## Download, unzip and load the data
download.file(url = 'https://raw.githubusercontent.com/GeoScripting-WUR/VectorRaster/gh-pages/data/landsat8.zip', destfile = 'data/landsat8.zip', method = 'auto')

unzip('data/landsat8.zip', exdir = "data")
## Identify the right file
landsatPath <- list.files(path = "data/", pattern = glob2rx('LC8*.grd'), full.names = TRUE)
wagLandsat <- brick(landsatPath)
```

```{r, echo=FALSE}
library(raster)
library(sf)
wagLandsat <- brick('data/LC81970242014109LGN00.grd')
```

We can start by visualizing the data. Since it is a multispectral image, we can use `plotRGB()` to do so.


```{r, fig.align='center'}
# plotRGB does not support negative values, so they need to be removed
wagLandsat[wagLandsat < 0] <- NA
names(wagLandsat) <- c("band1","band2","band3","band4","band5","band6","band7") # band names can be changed here
plotRGB(wagLandsat, 5, 4, 3) # select which bands to assign to the red, green, and blue colour channels
```

```{r}
## Download municipality boundaries
nlCity <- raster::getData('GADM',country='NLD', level=2)
class(nlCity)

## Convert nlCity to an sf object
nlCitySf <- sf::st_as_sf(nlCity)

## Investigate the structure of the object
head(nlCitySf)
```

It seems that the municipality names are in the `NAME_2` column. So we can subset the `sf data.frame` to the city of Wageningen alone. To do so we can use simple data frame manipulation/subsetting syntax.


```{r}
nlCitySf <- nlCitySf[!is.na(nlCitySf$NAME_2),] # Remove rows with NA

wagContour <- nlCitySf[nlCitySf$NAME_2 == 'Wageningen',] # Filter Wageningen
```

We can use the resulting `wagContour` object, to mask the values out of Wageningen, but first, since the two objects are in different coordinate systems, we need to reproject the projection of one to the other.

```{block, type="alert alert-success"}
> **Question 1:** Would you rather reproject a raster or a vector layer? Give two reasons why you would choose to reproject a raster or vector.
```

```{r, message=FALSE}
## Get the target CRS from the wagLandsat raster object
targetCRS <- sf::st_crs(wagLandsat)

## use sf to transform wagContour to the same projection as wagLandsat
wagContourUTM <- sf::st_transform(wagContour, targetCRS)
```

Now that the two objects are in the same CRS, we can do the masking and visualize the result. Let's first crop and then mask, to see the difference.

```{r, fig.align='center', fig.show='hold'}
wagLandsatCrop <- crop(wagLandsat, wagContourUTM)
wagLandsatSub <- mask(wagLandsat, wagContourUTM)

## Set graphical parameters (one row and two columns)
opar <- par(mfrow=c(1,2))
plotRGB(wagLandsatCrop, 5, 4, 3)
plotRGB(wagLandsatSub, 5, 4, 3)
plot(wagContourUTM, add = TRUE, border='green', col='transparent', lwd = 3) # set fill colour to transparent
## Reset graphical parameters
par(opar)
```

In the figure above, the left panel displays the output of `crop`, while the second panel shows the result of masking the Landsat scene using the contour of Wageningen as input.

We also have a water mask of Wageningen in vector format. Let's download it and also reproject it to the CRS of the Landsat data.

```{r, eval=FALSE}
download.file(url = 'https://raw.githubusercontent.com/GeoScripting-WUR/VectorRaster/gh-pages/data/wageningenWater.zip', destfile = 'data/wageningenWater.zip', method = 'auto')
unzip('data/wageningenWater.zip', exdir = "data")

## Load the Water Shapefile directly as an sf object
water <- sf::st_read('data/Water.shp')
names(water) ## check attribute columns

## Transform the water object to match the projection of our other data
waterUTM <- sf::st_transform(water, targetCRS)
```

```{r, echo=FALSE}
water <- sf::st_read('data/Water.shp')
waterUTM <- sf::st_transform(water, targetCRS)
```

Note the use of `inverse = TRUE` in the code below, to *mask* the pixels that intersect with the features of the vector object.


```{r, fig.align='center'}
wagLandsatSubW <- mask(wagLandsatSub, mask = waterUTM, inverse = TRUE)
plotRGB(wagLandsatSubW, 5, 4, 3)
plot(waterUTM, col = 'lightblue', add = TRUE, border = '#3A9AF0', lwd = 1)   # use a site such as https://htmlcolorcodes.com/ to find the hexidecimal code for colours
```

For the rest of the example, we'll use the `wagLandsatCrop` object, for we have a few doubts about the spatial accuracy of the two vector layers we used in the masking steps. You can check for yourself by converting them to KML and opening them in Google My Maps. (Let us know during the lesson, what do you think? Any solutions?)

### Build a calibration dataset in Google Maps
Below we'll see how we can deal with a calibration dataset in R, for calibrating a classifier. It involves working with `sf data.frame` classes, extracting reflectance data for the corresponding calibration samples, manipulating data frames and building a model that can be used to predict land cover. But first we need to build the calibration dataset (ground truth), and for that we will use Google Maps.

To begin with, make a KML of the study area extent which we will use as a guide.

```{r, echo=FALSE}
# get bounding box of cropped raster and export it as KML
wagLandsatCropBbox <- sf::st_as_sfc(st_bbox(wagLandsatCrop))
st_write(wagLandsatCropBbox, "wagLandsatCropBbox.kml", driver = "kml", delete_dsn=TRUE)
```

Open [Google My Maps](https://www.google.com/maps/about/mymaps/), click *get started*, login on your Google account, create a new map by clicking on the *+* sign, name the map training_landcover, under *Untitled layer* click *Import* and find the KML file that you just created. You will see a rectangle of the study area appear on the map with the name *wagLandsatCropBbox*, click *Add layer*, name the new, untitled layer *landcover_points*, change the basemap to a satellite map, click *Add marker*, draw points on top of a few landcover types, and name the point as the land cover type. Keep your points within the bounding box (the *wagLandsatCropBbox* layer), otherwise they are out of the extent of the Landsat tile. Keep it to a few classes, such as `agriculture`, `forest`, `water`, `urban`. When you are done (15 - 30 points, with at least 5 points for each class), export the file to KML.

**Note:** in the approach described above, we get to decide where the calibration samples are. Another approach would be to automatically generate randomly distributed samples. This can be done very easily in R using the `sf::st_sample()` function, which automatically returns a `sfc` (simple feature collection) object of any given size. Options for the `sf::st_sample()` include `type = regular` (for regular sampling) and `by_polygon = TRUE` for stratified sampling, to ensure that all classes (of a landcover classification) are equally represented in the sample using `multipolygon` delimiters. [See the documentation](https://www.rdocumentation.org/packages/sf/versions/0.9-6/topics/st_sample) for a more detailed explanation.

Load the newly created KML file using the `st_read()` function.

```{r}
samples <- sf::st_read(dsn = './data/sampleLandcover.kml')
```

Okay, nice, the data has been read as a simple feature. Re-project the object to the CRS of the Landsat data.

```{r}
## Re-project sf data.frame
samplesUTM <- sf::st_transform(samples, targetCRS)
```

### Calibrate the classifier
```{r}
## Extract the surface reflectance
calib <- raster::extract(wagLandsatCrop, samplesUTM, df=TRUE) ## df=TRUE i.e. return as a data.frame

## Combine the newly created data.frame to the description column of the calibration dataset
calib2 <- cbind(samplesUTM$Name, calib)

## Change the name of the first column, for convenience
colnames(calib2)[1] <- 'lc'

## Make the lc column into a factor
calib2$lc <-as.factor(calib2$lc)

## Inspect the structure of the data.frame
str(calib2)
```

**Note:** the use of `df = TRUE` in the `extract()` call is so that we get a data frame in return. Data frame is the most common class to work with all types of models, such as linear models (`lm()`) or random forest models as we use later.

Now we will calibrate a random forest model using the extracted data frame.
Do not focus too much on the algorithm used, the important part for this tutorial is the data extraction and the following data frame manipulation. More details will come about random forest classifiers tomorrow.


```{r, message = FALSE}
if(!require(randomForest)) {
  install.packages("randomForest")
}
library(randomForest)
## Calibrate model
model <- randomForest(lc ~ band1 + band2 + band3 + band4 + band5 + band6 + band7, data = calib2)
## Use the model to predict land cover
lcMap <- predict(wagLandsatCrop, model = model)
```

Let's visualize the output. The function `levelplot()` from the rasterVis package is a convenient function to plot categorical raster data.

```{r, fig.align='center'}
library(rasterVis)
levelplot(lcMap, main = "Landcover Map of Wageningen", col.regions = c('lightgreen', 'darkgreen', 'orange', 'blue'))
```


OK, we've seen better land cover maps of Wageningen, but given the amount of training data we used (22 in my case), it is not too bad. A larger calibration dataset would certainly result in a better accuracy.

## Extract raster values along a transect

Another use of the `extract()` function can be to visualize or analyse data along transects.
In the following example, we will run a transect across Belgium and visualize the change in elevation.

Let's first download the elevation data of Belgium, using the `getData()` function of the raster package.

```{r}
## Download data
bel <- getData('alt', country='BEL', mask=TRUE)
## Display metadata
bel
```

`bel` is a `RasterLayer`.

We can start by visualizing the data.

```{r, fig.align='center'}
plot(bel)
```

Everything seems correct.

We want to look at a transect, which we can draw by hand by selecting two points by clicking. The `drawLine()` function will help us do that. Once you run the function, you will be able to click in the plotting window of R (The `bel` object should already be present in the plot panel before running `drawLine()`). Click *Finish* or right-click on the plot once you have selected the two extremities of the line.


```{r, eval=FALSE}
line <- drawLine()
```

```{r, echo=FALSE, fig.align='center'}
line <- readRDS('data/line.rds')
plot(bel)
plot(line, add=TRUE)
```

Then the elevation values can simply be extracted using `raster::extract()`. Note the use of the `along=` argument which ensures that the samples remain in order along the segment.

```{r}
alt <- extract(bel, line, along = TRUE)
```

We can already plot the result as follows, but the x axis does not really provide any indication of distance.

```{r, fig.align='center'}
plot(alt[[1]], type = 'l', ylab = "Altitude (m)")
```

In order to make an index for the x axis, we can calculate the distance between the two extremities of the transect, using `distHaversine()` from the *geosphere* package.

```{r}
# check for the geosphere package and install if missing
if(!"geosphere" %in% rownames(installed.packages())){install.packages("geosphere")}
```

```{r}
library(geosphere)

## convert line to sf and assign the crs
line2 <- st_as_sf(line)
st_crs(line2) <- 4326

## Use sf to calculate the Great Circle distance between the two ends of the line
dist_sf <- st_length(line2,
                   which = ifelse(isTRUE(st_is_longlat(line2)), "Great Circle", "Euclidean")) ## st_is_longlat() returns true so which='Great Circle' option is used


## Use geosphere to calculate the Great Circle distance between the two ends of the line
start <- sf::st_coordinates(line2)[1,1:2] ## get the coordinate pairs for the start and end of the line
end <-   sf::st_coordinates(line2)[2,1:2]

dist_geosphere <- distHaversine(start,end)

## print the results
writeLines(c(paste0(as.integer(dist_sf), "m - sf Great Circle [WGS coords]"), paste0(as.integer(dist_geosphere), "m - geosphere Great Circle [WGS coords]")))

```
```{block, type="alert alert-success"}
> **Question 2:** Why is there a difference between the line lengths calculated by *sf* and *geosphere*?

*Hint:* Look at the help page of the `distHaversine()` function. Also see the `distVincentyEllipsoid()` function. Does this one provide a more accurate distance?
```

Note that there is a small approximation on the position of the line and the distances between the samples as the real shortest path between two points is not a straight line in lat-long, while the distance we just calculated is the shortest path. For short distances, as in the example, this is acceptable.
Otherwise, we could also have projected the raster and the line to a projected coordinate system.

Calculating long distances accurately on an imperfect spheroid is very tricky. The *geosphere distHaversine()* assumes a perfectly spherical projection. Take care when using these functions that you understand how the distance is being calculated and what implications that may have for your work.

We will continue using the distance calculated by *sf*.

```{r}
## Format an array for use as x axis index with the same length as the alt[[1]] array
distanceVector <- seq(0, as.numeric(dist_sf), along.with = alt[[1]])
```

Let's now visualize the final output.

```{r, fig.align='center', fig.show='hold'}
## Visualize the output
plot(bel, main = 'Altitude (m)')
plot(line, add = TRUE)
plot(distanceVector/1000, alt[[1]], type = 'l',
     main = 'Altitude transect Belgium',
     xlab = 'Distance (Km)',
     ylab = 'Altitude (m)',
     las = 1)

```

## Extract raster values randomly using the `sampleRandom()` function

Below is an extra example and a simplification of the random sample demo shown [here](https://geoscripting-wur.github.io/Scripting4Geo/). We will use the `sampleRandom()` function to randomly sample altitude information from a DEM of Belgium:

```{r}
# You can choose your own country here
bel <- getData('alt', country='BEL', mask=TRUE) ## SRTM 90m height data
belAdmin <- getData('GADM', country='BEL', level=2) ## administrative boundaries

## Sample the raster randomly with 40 points
sRandomBel <- sampleRandom(bel, na.rm=TRUE, sp=TRUE, size = 40)

## Create a data.frame containing relevant info
sRandomData <- data.frame(altitude = sRandomBel@data[[1]],
                          latitude = sRandomBel@coords[,'y'],
                          longitude = sRandomBel@coords[,'x'])
```

```{r, fig.align='center', fig.show='hold'}
## Plot
plot(bel)
plot(belAdmin, add=TRUE)
plot(sRandomBel, add = TRUE, col = "red")

## Plot altitude versus latitude
plot(sRandomData$latitude,sRandomData$altitude, ylab = "Altitude (m)", xlab = "Latitude (degrees)")
```

```{block, type="alert alert-success"}
> **Question 3**: If you would like to sample height data per province randomly, what would you need to do?
```

<!--

# Exercise 7: The greenest municipality

## Background
Last week end, on a rainy Saturday afternoon in Wageningen, my friend and I decided to go for a coffee. We had met at the market, while shopping for fresh vegetables, when we thought of moving to a warmer and drier place to catch up a bit since last time we had met a month ago. We were both sitting in the cafe; me by the window and he, sitting on the opposite side of the table, facing the rest of the busy room. The chattering of the crown behind me, mixed with the clinking of glasses and the hissing sound of the espresso machine releasing its vapour was giving the feeling of an old movie to the place. The rain beating the window was only adding to this atmosphere. As we were discussing, I was contemplating the rain drops rolling down the glass, on the other side of the window; gaining speed and chasing each others on the smooth surface, then being pushed out of their trajectory by a sudden burst of wind, and gaining momentum again. He suddently exclaimed:

*'Did you know that Hoenderloo is the greenest city in the Netherlands?'*

I was shocked, not only this had absolutely nothing to do with our conversation, but also I had always thought that Wageningen was the greenest city in The Netherlands. At least that's what I wanted to believe. So that I responded:

*'No way man, it can't be greener than Wageningen. Wageningen is way greener than Hoenderloo.'* Insisting on the "way" and pronouncing Hoenderloo in an English way.

*'Are you sure?'*

*'Definitely!'* I had no idea.

To what he responded, noticing my bluffing face:

*'Haha, if only we could check.'*

''If only ...'', these two words kept bouncing in my mind. So much that I could not follow the rest of our conversation. Whether he was talking about the influence of China on Wageningen architecture, the maximum speed of a Leopard chasing a Gazelle, his last trip to Bolivia or the true origin of the french fries name, I do not remember. I was obsessed by this thought; how could I find which was the greenest place? He did most of the talking, while my mind was trying to organize the ideas and sort preliminary hypotheses. When suddently he got my attention again:

*'Hey let's go!'*

The rain had stopped. We got up, paid our coffees to the counter, exited the cafe and each started walking in a different direction. I had almost reached the end of the street when I turned around and shouted in his direction.

*'I think we can check!'*

*'Check what?'*

I gave him a sign with my arm indicating that it didn't matter and kept walking home.

## Your task

Please help me find out which "municipality" in the Netherlands was the greenest:

* in January;
* in August;
* on average over the year.

Visualize the results using maps with proper titles, labels and legends.

## More details

* You can use the MODIS NDVI data available [here](https://raw.githubusercontent.com/GeoScripting-WUR/VectorRaster/gh-pages/data/MODIS.zip).
* [Click here for more information about MODIS data used (i.e. MOD13A3)](https://lpdaac.usgs.gov/dataset_discovery/modis/modis_products_table/mod13a3_v006)

```{block, type="alert alert-info"}
**Hint:** Use `nlMunicipality <- getData('GADM',country='NLD', level=2)`
```

## Assessment

The general rubrics will be used for the assessment of your script. These can be found on BlackBoard. The three specific tasks in the rubrics your script will be assessed on are:

Task1: Find the greenest municipality in January and August.

Task2: Find the greenest municipality on average over the year.

Task3: Visualize the results in at least one plot with proper title, legend and labels.


## Bonus

You can earn the bonus points by finding and mapping the greenest province in January.

```{block, type="alert alert-info"}
**Hint:** See `?raster::aggregate`
```

## Submission

Make sure your script is clear and reproducible. Make a new GitLab project and upload the script there according to the guidelines on BlackBoard. Review the script of another team (see the random team selector Shiny app) tomorrow morning and add an issue t their project with your feedback.

Do not forget to add your teamname, members and date at the top of your script.
-->
